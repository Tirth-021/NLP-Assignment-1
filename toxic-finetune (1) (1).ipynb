{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121  # for GPU (CUDA 12.1)\n",
    "# or: !pip install torch  # if you’re on CPU only\n",
    "\n",
    "!pip install -q transformers\n",
    "!pip install -q datasets\n",
    "!pip install -q huggingface_hub\n",
    "!pip install -q peft\n",
    "!pip install -q trl\n",
    "!pip install -q accelerate\n",
    "!pip install -q bitsandbytes\n",
    "!pip install -q numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from huggingface_hub import login\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import DPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters to set ---\n",
    "\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" \n",
    "\n",
    "\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "\n",
    "ALL_DATASET_NAMES = [\n",
    "    \"panda\",\n",
    "    \"jigsaw\",\n",
    "    \"biasdpo\",\n",
    "    \"detoxdpo\",\n",
    "    \"biassft\",\n",
    "    \"detoxsft\",\n",
    "]\n",
    "\n",
    "# Random seed for reproducibility\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be defined after the tokenizer is loaded\n",
    "tokenizer = None\n",
    "context_length = 512\n",
    "\n",
    "def tokenize_panda(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"perturbed\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        padding=\"max_length\",\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for input_ids in outputs[\"input_ids\"]:\n",
    "        input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "def tokenize_jigsaw(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"comment_text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        padding=\"max_length\",\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for input_ids in outputs[\"input_ids\"]:\n",
    "        input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "def tokenize_sft(element):\n",
    "    text = [\n",
    "        element[\"prompt\"][i] + \" \" + element[\"chosen\"][i]\n",
    "        for i in range(len(element[\"prompt\"]))\n",
    "    ]\n",
    "    outputs = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        padding=\"max_length\",\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for input_ids in outputs[\"input_ids\"]:\n",
    "        input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409e4c0e757d4f40a43cf2cb19d32e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202a2380492846fc8854d472f1ee1f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b130d8d07949f68d87666bcba74ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33398e7c2dde482b876cedfc7028a112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f158e7d6350455287ff15190fc02d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca290bba1f3541b79beebe83b7a0aa08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676cd722ae9844c59f82ac5444a3814b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e2b23eb2f54aaebc774de676c4b5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35118d34dfee4845bcb1ce802f41dc9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98eb192d0ea1476dab2b1e154d9975d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e9f211eee44f85b9e459bb09e6f4dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae1b664db014b6795b6dcdd5eb92349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MODEL LOADED\n"
     ]
    }
   ],
   "source": [
    "# QLoRA configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")\n",
    "\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=\"all-linear\",\n",
    "    bias=\"none\",\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if not tokenizer.pad_token:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "if \"gemma\" in MODEL_NAME:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"eager\",\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "print(\"✅ MODEL LOADED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_panda(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"perturbed\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        padding=\"max_length\",\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    return {\"input_ids\": outputs[\"input_ids\"]}\n",
    "\n",
    "def tokenize_jigsaw(element):\n",
    "     # ensure text is always a string, handle NaNs\n",
    "    texts = [str(x) if x is not None else \"\" for x in element[\"comment_text\"]]\n",
    "\n",
    "    outputs = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        padding=\"max_length\",\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "\n",
    "    return {\"input_ids\": outputs[\"input_ids\"], \"attention_mask\": outputs[\"attention_mask\"]}\n",
    "\n",
    "def tokenize_sft(element):\n",
    "    text = [element[\"prompt\"][i] + \" \" + element[\"chosen\"][i] for i in range(len(element[\"prompt\"]))]\n",
    "    outputs = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        padding=\"max_length\",\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    return {\"input_ids\": outputs[\"input_ids\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainer, output_dir):\n",
    "    print(f\"🚀 Starting training for {output_dir}...\")\n",
    "    trainer.train()\n",
    "    final_model = trainer.model.merge_and_unload()\n",
    "    final_model.save_pretrained(output_dir)\n",
    "    print(f\"✅ Model saved at: {output_dir}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base_model():\n",
    "    if \"gemma\" in MODEL_NAME:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            attn_implementation=\"eager\",\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "    model.enable_input_require_grads()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# Helper: Save trained model (try merge_and_unload then fallback)\n",
    "# -------------------\n",
    "import os\n",
    "def safe_save_trained_model(trainer, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    try:\n",
    "        # many PEFT workflows have merge_and_unload()\n",
    "        model = trainer.model\n",
    "        if hasattr(model, \"merge_and_unload\"):\n",
    "            merged = model.merge_and_unload()\n",
    "            merged.save_pretrained(output_dir)\n",
    "        else:\n",
    "            # fallback\n",
    "            trainer.model.save_pretrained(output_dir)\n",
    "    except Exception as e:\n",
    "        # final fallback: try to unwrap or just save weights\n",
    "        try:\n",
    "            trainer.model.save_pretrained(output_dir)\n",
    "        except Exception as e2:\n",
    "            raise RuntimeError(f\"Could not save model: {e} / {e2}\")\n",
    "\n",
    "# -------------------\n",
    "# Small utility wrapper to train and save\n",
    "# -------------------\n",
    "def train_and_save(trainer, out_dir):\n",
    "    print(f\"\\n--- START TRAIN: {out_dir} ---\")\n",
    "    trainer.train()\n",
    "    safe_save_trained_model(trainer, out_dir)\n",
    "    print(f\"✅ Saved to {out_dir}\")\n",
    "\n",
    "    # cleanup\n",
    "    del trainer\n",
    "    try:\n",
    "        del model\n",
    "    except:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "    import gc; gc.collect()\n",
    "    print(f\"🧹 Freed memory for {out_dir}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing PANDA (SFT) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f67fb806eb4d498bf90a83ba23bbcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935bb8917a144050984f8c7ff39bfd07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.jsonl:   0%|          | 0.00/194M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea447ef51fd74ae7ad6b418e78db3936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid.jsonl:   0%|          | 0.00/21.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1b3f16c8504fd28ddbb6a0727f4a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/94966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7852d13619a0407fbb532288fff6c322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c470e199755453e94246f16b5fd1bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/94966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#model = load_base_model()\u001b[39;00m\n\u001b[1;32m     46\u001b[0m raw \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/panda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m tokenized \u001b[38;5;241m=\u001b[39m \u001b[43mraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenize_panda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# for notebooks / quick debugging we use smaller subsets; remove .select for full training\u001b[39;00m\n\u001b[1;32m     52\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m tokenized[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtrain_test_split(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, seed\u001b[38;5;241m=\u001b[39mSEED)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/datasets/dataset_dict.py:954\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc, try_original_type)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[1;32m    952\u001b[0m     function \u001b[38;5;241m=\u001b[39m bind(function, split)\n\u001b[0;32m--> 954\u001b[0m dataset_dict[split] \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_original_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_original_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[1;32m    976\u001b[0m     function \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunc\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/datasets/arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/datasets/arrow_dataset.py:3327\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[1;32m   3325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3326\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m unprocessed_kwargs \u001b[38;5;129;01min\u001b[39;00m unprocessed_kwargs_per_job:\n\u001b[0;32m-> 3327\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munprocessed_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3328\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcheck_if_shard_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3330\u001b[0m \u001b[38;5;66;03m# Avoids PermissionError on Windows (the error: https://github.com/huggingface/datasets/actions/runs/4026734820/jobs/6921621805)\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/datasets/arrow_dataset.py:3698\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[0m\n\u001b[1;32m   3696\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_table(batch\u001b[38;5;241m.\u001b[39mto_arrow())\n\u001b[1;32m   3697\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3698\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_original_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_original_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3699\u001b[0m num_examples_progress_update \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_examples_in_batch\n\u001b[1;32m   3700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m _time \u001b[38;5;241m+\u001b[39m config\u001b[38;5;241m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/datasets/arrow_writer.py:697\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[0;34m(self, batch_examples, writer_batch_size, try_original_type)\u001b[0m\n\u001b[1;32m    691\u001b[0m         col_try_type \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    692\u001b[0m             try_features[col]\n\u001b[1;32m    693\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m try_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m try_features \u001b[38;5;129;01mand\u001b[39;00m try_original_type\n\u001b[1;32m    694\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         typed_sequence \u001b[38;5;241m=\u001b[39m OptimizedTypedSequence(col_values, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mcol_type, try_type\u001b[38;5;241m=\u001b[39mcol_try_type, col\u001b[38;5;241m=\u001b[39mcol)\n\u001b[0;32m--> 697\u001b[0m         arrays\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyped_sequence\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    698\u001b[0m         inferred_features[col] \u001b[38;5;241m=\u001b[39m typed_sequence\u001b[38;5;241m.\u001b[39mget_inferred_type()\n\u001b[1;32m    699\u001b[0m schema \u001b[38;5;241m=\u001b[39m inferred_features\u001b[38;5;241m.\u001b[39marrow_schema \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pyarrow/array.pxi:256\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pyarrow/array.pxi:118\u001b[0m, in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/datasets/arrow_writer.py:308\u001b[0m, in \u001b[0;36mTypedSequence.__arrow_array__\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     trying_cast_to_python_objects \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to_python_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monly_1d_for_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# use smaller integer precisions if possible\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrying_int_optimization:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -------------------\n",
    "# Helper: Save trained model (try merge_and_unload then fallback)\n",
    "# -------------------\n",
    "def safe_save_trained_model(trainer, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    try:\n",
    "        # many PEFT workflows have merge_and_unload()\n",
    "        model = trainer.model\n",
    "        if hasattr(model, \"merge_and_unload\"):\n",
    "            merged = model.merge_and_unload()\n",
    "            merged.save_pretrained(output_dir)\n",
    "        else:\n",
    "            # fallback\n",
    "            trainer.model.save_pretrained(output_dir)\n",
    "    except Exception as e:\n",
    "        # final fallback: try to unwrap or just save weights\n",
    "        try:\n",
    "            trainer.model.save_pretrained(output_dir)\n",
    "        except Exception as e2:\n",
    "            raise RuntimeError(f\"Could not save model: {e} / {e2}\")\n",
    "\n",
    "# -------------------\n",
    "# Small utility wrapper to train and save\n",
    "# -------------------\n",
    "def train_and_save(trainer, out_dir):\n",
    "    print(f\"\\n--- START TRAIN: {out_dir} ---\")\n",
    "    trainer.train()\n",
    "    safe_save_trained_model(trainer, out_dir)\n",
    "    print(f\"✅ Saved to {out_dir}\")\n",
    "\n",
    "    # cleanup\n",
    "    del trainer\n",
    "    try:\n",
    "        del model\n",
    "    except:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "    import gc; gc.collect()\n",
    "    print(f\"🧹 Freed memory for {out_dir}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# ---------- 1) SFT on Panda (bias mitigation) ----------\n",
    "print(\"Preparing PANDA (SFT) ...\")\n",
    "#model = load_base_model()\n",
    "raw = load_dataset(\"facebook/panda\")\n",
    "\n",
    "tokenized = raw.map(\n",
    "    tokenize_panda, batched=True, remove_columns=raw[\"train\"].column_names\n",
    ")\n",
    "# for notebooks / quick debugging we use smaller subsets; remove .select for full training\n",
    "train_ds = tokenized[\"train\"].train_test_split(test_size=0.5, seed=SEED)[\"train\"]\n",
    "eval_ds = tokenized[\"validation\"].train_test_split(test_size=0.5, seed=SEED)[\"train\"]\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"panda_lora_model\",\n",
    "    num_train_epochs=1,\n",
    "            save_total_limit=5,\n",
    "            eval_strategy=\"steps\",\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            warmup_steps=150,\n",
    "            weight_decay=0.001,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            fp16=True,\n",
    "            remove_unused_columns=False,\n",
    "            logging_steps=500,\n",
    "            eval_steps=500,\n",
    "            save_steps=500,\n",
    "            save_strategy=\"steps\",\n",
    "            learning_rate=3e-4,\n",
    "            gradient_checkpointing=True,\n",
    "            gradient_checkpointing_kwargs={\"use_reentrant\": True},\n",
    "            load_best_model_at_end=True,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "train_and_save(trainer, \"panda_lora_model\")\n",
    "\n",
    "print(\"Fine tuning done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading jigsaw-unintended-bias-in-toxicity-classification.zip to jigsaw_data\n",
      " 79%|███████████████████████████████▋        | 574M/723M [00:00<00:00, 1.98GB/s]\n",
      "100%|████████████████████████████████████████| 723M/723M [00:00<00:00, 1.98GB/s]\n",
      "Archive:  jigsaw_data/jigsaw-unintended-bias-in-toxicity-classification.zip\n",
      "  inflating: jigsaw_data/all_data.csv  \n",
      "  inflating: jigsaw_data/identity_individual_annotations.csv  \n",
      "  inflating: jigsaw_data/sample_submission.csv  \n",
      "  inflating: jigsaw_data/test.csv    \n",
      "  inflating: jigsaw_data/test_private_expanded.csv  \n",
      "  inflating: jigsaw_data/test_public_expanded.csv  \n",
      "  inflating: jigsaw_data/toxicity_individual_annotations.csv  \n",
      "  inflating: jigsaw_data/train.csv   \n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c jigsaw-unintended-bias-in-toxicity-classification -p jigsaw_data\n",
    "!unzip -o jigsaw_data/jigsaw-unintended-bias-in-toxicity-classification.zip -d jigsaw_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing JIGSAW (SFT) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_3153/1502060526.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- START TRAIN: jigsaw_lora_model ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 12:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.101700</td>\n",
       "      <td>3.021605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.018900</td>\n",
       "      <td>2.824741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 53\u001b[0m\n\u001b[1;32m     44\u001b[0m model \u001b[38;5;241m=\u001b[39m get_peft_model(model, peft_config)\n\u001b[1;32m     45\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     46\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     47\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     52\u001b[0m )\n\u001b[0;32m---> 53\u001b[0m \u001b[43mtrain_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjigsaw_lora_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 28\u001b[0m, in \u001b[0;36mtrain_and_save\u001b[0;34m(trainer, out_dir)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- START TRAIN: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 28\u001b[0m \u001b[43msafe_save_trained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# cleanup\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[26], line 5\u001b[0m, in \u001b[0;36msafe_save_trained_model\u001b[0;34m(trainer, output_dir)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msafe_save_trained_model\u001b[39m(trainer, output_dir):\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# many PEFT workflows have merge_and_unload()\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         model \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mmodel\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Preparing JIGSAW (SFT) ...\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "# Load Jigsaw train.csv only\n",
    "raw = load_dataset(\"csv\", data_files={\"train\": \"jigsaw_data/train.csv\"})\n",
    "\n",
    "# keep non-toxic comments per the paper (target < 0.1)\n",
    "raw = raw.filter(lambda ex: ex[\"target\"] < 0.1)\n",
    "\n",
    "tokenized = raw.map(\n",
    "    tokenize_jigsaw, batched=True, remove_columns=raw[\"train\"].column_names\n",
    ")\n",
    "train_ds = tokenized[\"train\"].select(range(min(5000, len(tokenized[\"train\"]))))\n",
    "# paper uses test_private_leaderboard as validation\n",
    "valid_key = \"test_private_leaderboard\" if \"test_private_leaderboard\" in tokenized else list(tokenized.keys())[-1]\n",
    "eval_ds = tokenized[valid_key].select(range(min(1000, len(tokenized[valid_key]))))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"jigsaw_lora_model\",\n",
    "     num_train_epochs=1,\n",
    "            save_total_limit=5,\n",
    "            eval_strategy=\"steps\",\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            warmup_steps=150,\n",
    "            weight_decay=0.001,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            fp16=True,\n",
    "            remove_unused_columns=False,\n",
    "            logging_steps=500,\n",
    "            eval_steps=500,\n",
    "            save_steps=500,\n",
    "            save_strategy=\"steps\",\n",
    "            learning_rate=3e-4,\n",
    "            gradient_checkpointing=True,\n",
    "            gradient_checkpointing_kwargs={\"use_reentrant\": True},\n",
    "            load_best_model_at_end=True,\n",
    "    seed=SEED,\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "train_and_save(trainer, \"jigsaw_lora_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_base_model()\n",
    "model = get_peft_model(model, peft_config)   # create PEFT adapters now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRL version: 0.23.0\n",
      "Preparing BiasDPO (DPO) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e2b12d37304395a32fa2a267818c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/286 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7068a80f55ec44808277fc09cd4faf80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/286 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9b420b71514ba6b786e6da10bccb0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset:   0%|          | 0/286 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b894b5cbc00407ba329d30d4f6c97d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/286 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0879599fc64cf0a98829e036215e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/286 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8bb485cd4d48a6b7f7b74bc83c823b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in eval dataset:   0%|          | 0/286 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1185f39c274c4589aa63b8979d2fd108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/286 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb5849054c946b88647700830f2c719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/286 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- START TRAIN: biasdpo_lora_model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='72' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [72/72 00:34, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to biasdpo_lora_model\n",
      "🧹 Freed memory for biasdpo_lora_model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------- 3) DPO on BiasDPO (bias mitigation) ----------\n",
    "\n",
    "import trl\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "print(\"TRL version:\", getattr(trl, \"__version__\", \"unknown\"))\n",
    "\n",
    "print(\"Preparing BiasDPO (DPO) ...\")\n",
    "\n",
    "# 1) load and split dataset (keep 50% as you requested earlier)\n",
    "raw = load_dataset(\"ahmedallam/BiasDPO\")[\"train\"]\n",
    "ds = raw.train_test_split(test_size=0.5, seed=SEED)   # keep half\n",
    "train_d = ds[\"train\"].train_test_split(test_size=0.5, seed=SEED)[\"train\"]  # smaller subset\n",
    "eval_d  = ds[\"test\"].train_test_split(test_size=0.5, seed=SEED)[\"train\"]\n",
    "\n",
    "# def tokenize_dpo(batch):\n",
    "#     prompts = [str(x) if x is not None else \"\" for x in batch.get(\"prompt\", [])]\n",
    "#     chosen  = [str(x) if x is not None else \"\" for x in batch.get(\"chosen\", [])]\n",
    "#     rejected= [str(x) if x is not None else \"\" for x in batch.get(\"rejected\", [])]\n",
    "\n",
    "#     # prompt truncated to prompt length, completions to max_length\n",
    "#     p = tokenizer(prompts, truncation=True, max_length=128, padding=\"max_length\", return_attention_mask=True)\n",
    "#     c = tokenizer(chosen,  truncation=True, max_length=512, padding=\"max_length\", return_attention_mask=True)\n",
    "#     r = tokenizer(rejected,truncation=True, max_length=512, padding=\"max_length\", return_attention_mask=True)\n",
    "\n",
    "#     return {\n",
    "#         \"prompt_input_ids\": p[\"input_ids\"],\n",
    "#         \"prompt_attention_mask\": p[\"attention_mask\"],\n",
    "#         \"chosen_input_ids\": c[\"input_ids\"],\n",
    "#         \"chosen_attention_mask\": c[\"attention_mask\"],\n",
    "#         \"rejected_input_ids\": r[\"input_ids\"],\n",
    "#         \"rejected_attention_mask\": r[\"attention_mask\"],\n",
    "#     }\n",
    "\n",
    "columns_to_remove = [col for col in train_d.column_names if col in ['prompt_input_ids', 'chosen_input_ids', 'rejected_input_ids']]\n",
    "\n",
    "# 3) apply tokenization (batched)\n",
    "train_tok = train_d.map(tokenize_dpo, batched=True, remove_columns=columns_to_remove)\n",
    "eval_tok  = eval_d.map(tokenize_dpo,  batched=True, remove_columns=columns_to_remove)\n",
    "\n",
    "\n",
    "training_args = DPOConfig(\n",
    "    output_dir=\"biasdpo_lora_model\",\n",
    "    num_train_epochs=1,\n",
    "            save_total_limit=5,\n",
    "            eval_strategy=\"steps\",\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            warmup_steps=150,\n",
    "            weight_decay=0.001,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            fp16=True,\n",
    "            remove_unused_columns=False,\n",
    "            logging_steps=500,\n",
    "            eval_steps=500,\n",
    "            save_steps=500,\n",
    "            save_strategy=\"steps\",\n",
    "            learning_rate=3e-4,\n",
    "            gradient_checkpointing=True,\n",
    "            gradient_checkpointing_kwargs={\"use_reentrant\": True},\n",
    "            load_best_model_at_end=True,\n",
    "    seed=SEED,\n",
    ")\n",
    "#training_args = DPOConfig(output_dir=\"Qwen2.5-0.5B-DPO\")\n",
    "\n",
    "training_args.padding_value = tokenizer.pad_token_id\n",
    "#training_args.model_init_kwargs = {}\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=eval_tok,\n",
    "    processing_class=tokenizer,  # still allowed\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)],\n",
    ")\n",
    "\n",
    "\n",
    "# 8) train & save (uses your train_and_save / safe_save_trained_model)\n",
    "train_and_save(dpo_trainer, \"biasdpo_lora_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing DetoxDPO (DPO) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/peft/mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'meta-llama/Meta-Llama-3.1-8B-Instruct' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccac5c8aab344ca8a8aea0eb2d6fb0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252edb476d544719917cc385c7f65c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Converted split 'train' to JSONL\n",
      "✅ Converted split 'validation' to JSONL\n",
      "\n",
      "--- START TRAIN: detoxdpo_lora_model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='72' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [72/72 02:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ---------- 4) DPO on DetoxDPO (toxicity mitigation) ----------\n",
    "print(\"Preparing DetoxDPO (DPO) ...\")\n",
    "import os\n",
    "#model = load_base_model()\n",
    "model = get_peft_model(model, peft_config) \n",
    "# load the toxicity_pairwise files (paper uses 6 splits). If not present locally, you should download them.\n",
    "# For robustness, try up to 6 files and stop if file missing.\n",
    "seed = 42\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"your-model-name\")\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"DopeorNope/New_DPO_dataset\")\n",
    "\n",
    "# Convert to JSONL and save\n",
    "for split in dataset.keys():\n",
    "    dataset[split].to_json(f\"toxicity_pairwise/split_{split}.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "for split_name, split_data in dataset.items():\n",
    "    # split_data is a Dataset, which has .to_pandas()\n",
    "    df = split_data.to_pandas()\n",
    "    df.to_json(f\"toxicity_pairwise/split_{split_name}.jsonl\", orient=\"records\", lines=True)\n",
    "    print(f\"✅ Converted split '{split_name}' to JSONL\")\n",
    "    \n",
    "# Prepare the dataset\n",
    "#df = dataset.to_pandas()\n",
    "df = df.rename(columns={\"prompt_text\": \"prompt\", \"unpert_gen_text\": \"chosen\", \"pert_gen_text\": \"rejected\"})\n",
    "dataset = Dataset.from_pandas(df).train_test_split(test_size=0.3, seed=seed)\n",
    "\n",
    "train_d = dataset[\"train\"].select(range(min(1000, len(dataset[\"train\"]))))\n",
    "eval_d = dataset[\"test\"].select(range(min(200, len(dataset[\"test\"]))))\n",
    "\n",
    "training_args = DPOConfig(\n",
    "    output_dir=\"detoxdpo_lora_model\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    save_steps=100,\n",
    "    learning_rate=1e-5,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=eval_tok,\n",
    "    processing_class=tokenizer,  # still allowed\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=10)],\n",
    ")\n",
    "\n",
    "train_and_save(dpo_trainer, \"detoxdpo_lora_model\")\n",
    "\n",
    "print(\"🎉 All pipelines finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T08:54:03.353230Z",
     "iopub.status.busy": "2025-09-27T08:54:03.352625Z",
     "iopub.status.idle": "2025-09-27T08:55:00.312597Z",
     "shell.execute_reply": "2025-09-27T08:55:00.311845Z",
     "shell.execute_reply.started": "2025-09-27T08:54:03.353205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7248bba39b0a4f09a28d15d70d28d19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b193a6db83b041ad85f079f95395757f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.jsonl:   0%|          | 0.00/194M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08762d1e0b8f4dd08fa5808396109e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid.jsonl:   0%|          | 0.00/21.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b127259444041b692fee5c5000163eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/94966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6f5fd93e8d429692d647c9e698bf59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c64510cb554827be82f4045b6992d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/94966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6032764337d41a3af6fa478b0eb818b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DATASET 'panda' LOADED\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "#  SFT training on PANDA or Jigsaw\n",
    "# ===============================================================================\n",
    "if DATASET_NAME in [\"panda\", \"jigsaw\"]:\n",
    "    if DATASET_NAME == \"panda\":\n",
    "        raw_datasets = load_dataset(\"facebook/panda\")\n",
    "        valid_name = \"validation\"\n",
    "        tokenize_func = tokenize_panda\n",
    "    elif DATASET_NAME == \"jigsaw\":\n",
    "        raw_datasets = load_dataset(\n",
    "            \"jigsaw_unintended_bias\",\n",
    "            data_dir=\"jigsaw-unintended-bias-in-toxicity-classification\",\n",
    "        )\n",
    "        raw_datasets = raw_datasets.filter(\n",
    "            lambda example: example[\"target\"] < 0.1\n",
    "        )\n",
    "        valid_name = \"test_private_leaderboard\"\n",
    "        tokenize_func = tokenize_jigsaw\n",
    "    \n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_func,\n",
    "        batched=True,\n",
    "        remove_columns=raw_datasets[\"train\"].column_names,\n",
    "    )\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    print(f\"✅ DATASET '{DATASET_NAME}' LOADED\")\n",
    "\n",
    "# ===============================================================================\n",
    "# DPO training\n",
    "# ===============================================================================\n",
    "elif DATASET_NAME in [\"biasdpo\", \"detoxdpo\"]:\n",
    "    if DATASET_NAME == \"biasdpo\":\n",
    "        dataset = load_dataset(\"ahmedallam/BiasDPO\")[\n",
    "            \"train\"\n",
    "        ].train_test_split(test_size=0.05, seed=SEED)\n",
    "        num_epochs = 20\n",
    "        n_steps = 25\n",
    "    elif DATASET_NAME == \"detoxdpo\":\n",
    "        # NOTE: This requires local data files\n",
    "        dataset = pd.concat(\n",
    "            [\n",
    "                pd.read_json(\n",
    "                    f\"toxicity_pairwise/split_{i}.jsonl\", lines=True\n",
    "                )[[\"prompt_text\", \"unpert_gen_text\", \"pert_gen_text\"]]\n",
    "                for i in range(6)\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        ).rename(\n",
    "            columns={\n",
    "                \"prompt_text\": \"prompt\",\n",
    "                \"unpert_gen_text\": \"chosen\",\n",
    "                \"pert_gen_text\": \"rejected\",\n",
    "            }\n",
    "        )\n",
    "        num_epochs = 1\n",
    "        n_steps = 100\n",
    "        dataset = Dataset.from_pandas(dataset).train_test_split(\n",
    "            test_size=0.05, seed=SEED\n",
    "        )\n",
    "    print(f\"✅ DATASET '{DATASET_NAME}' LOADED\")\n",
    "\n",
    "# ===============================================================================\n",
    "# SFT training on DPO data\n",
    "# ===============================================================================\n",
    "elif DATASET_NAME in [\"biassft\", \"detoxsft\"]:\n",
    "    if DATASET_NAME == \"biassft\":\n",
    "        dataset = load_dataset(\"ahmedallam/BiasDPO\")[\n",
    "            \"train\"\n",
    "        ].train_test_split(test_size=0.05, seed=SEED)\n",
    "        num_epochs = 20\n",
    "        n_steps = 25\n",
    "    elif DATASET_NAME == \"detoxsft\":\n",
    "        # NOTE: This requires local data files\n",
    "        dataset = pd.concat(\n",
    "            [\n",
    "                pd.read_json(\n",
    "                    f\"toxicity_pairwise/split_{i}.jsonl\", lines=True\n",
    "                )[[\"prompt_text\", \"unpert_gen_text\", \"pert_gen_text\"]]\n",
    "                for i in range(6)\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        ).rename(\n",
    "            columns={\n",
    "                \"prompt_text\": \"prompt\",\n",
    "                \"unpert_gen_text\": \"chosen\",\n",
    "                \"pert_gen_text\": \"rejected\",\n",
    "            }\n",
    "        )\n",
    "        num_epochs = 1\n",
    "        n_steps = 100\n",
    "        dataset = Dataset.from_pandas(dataset).train_test_split(\n",
    "            test_size=0.05, seed=SEED\n",
    "        )\n",
    "\n",
    "    tokenized_datasets = dataset.map(\n",
    "        tokenize_sft,\n",
    "        batched=True,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "    )\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    print(f\"✅ DATASET '{DATASET_NAME}' LOADED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-27T09:20:47.275Z",
     "iopub.execute_input": "2025-09-27T08:55:41.896457Z",
     "iopub.status.busy": "2025-09-27T08:55:41.896151Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/4127178226.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128009}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 STARTING SFT TRAINING\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "#  SFT Training (panda, jigsaw)\n",
    "# ===============================================================================\n",
    "if DATASET_NAME in [\"panda\", \"jigsaw\"]:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{MODEL_NAME}_lora_{DATASET_NAME}\",\n",
    "        num_train_epochs=1,\n",
    "        save_total_limit=5,\n",
    "        eval_strategy=\"steps\",\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=150,\n",
    "        weight_decay=0.001,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        fp16=True,\n",
    "        remove_unused_columns=False,\n",
    "        logging_steps=500,\n",
    "        eval_steps=500,\n",
    "        save_steps=500,\n",
    "        save_strategy=\"steps\",\n",
    "        learning_rate=3e-4,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": True},\n",
    "        load_best_model_at_end=True,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"].select(range(94966)),\n",
    "        eval_dataset=tokenized_datasets[valid_name].select(range(10551)),\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    print(\"🚀 STARTING SFT TRAINING\")\n",
    "    trainer.train()\n",
    "    model = model.merge_and_unload()\n",
    "    model.save_pretrained(f\"{MODEL_NAME}_lora_{DATASET_NAME}_model\")\n",
    "\n",
    "# ===============================================================================\n",
    "#  DPO Training\n",
    "# ===============================================================================\n",
    "elif DATASET_NAME in [\"biasdpo\", \"detoxdpo\"]:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{MODEL_NAME}_lora_{DATASET_NAME}\",\n",
    "        optim=\"rmsprop\",\n",
    "        learning_rate=1e-5,\n",
    "        save_total_limit=5,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        remove_unused_columns=False,\n",
    "        num_train_epochs=num_epochs,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        logging_steps=n_steps,\n",
    "        save_steps=n_steps,\n",
    "        fp16=True,\n",
    "        eval_steps=n_steps,\n",
    "        eval_strategy=\"steps\",\n",
    "        lr_scheduler_type=\"constant_with_warmup\",\n",
    "        save_strategy=\"steps\",\n",
    "        warmup_steps=150,\n",
    "        weight_decay=0.05,\n",
    "        max_grad_norm=10,\n",
    "        load_best_model_at_end=True,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": True},\n",
    "        seed=SEED,\n",
    "    )\n",
    "    trainer = DPOTrainer(\n",
    "        model,\n",
    "        beta=0.1,\n",
    "        max_prompt_length=128,\n",
    "        max_length=512,\n",
    "        peft_config=peft_config,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=10)],\n",
    "    )\n",
    "    print(\"🚀 STARTING DPO TRAINING\")\n",
    "    trainer.train()\n",
    "    model = trainer.model.merge_and_unload()\n",
    "    model.save_pretrained(f\"{MODEL_NAME}_lora_{DATASET_NAME}_model\")\n",
    "    \n",
    "# ===============================================================================\n",
    "#  SFT Training (biassft, detoxsft)\n",
    "# ===============================================================================\n",
    "elif DATASET_NAME in [\"biassft\", \"detoxsft\"]:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{MODEL_NAME}_lora_{DATASET_NAME}\",\n",
    "        num_train_epochs=num_epochs,\n",
    "        save_total_limit=5,\n",
    "        eval_strategy=\"steps\",\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=n_steps,\n",
    "        weight_decay=0.001,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        fp16=True,\n",
    "        remove_unused_columns=False,\n",
    "        logging_steps=500,\n",
    "        eval_steps=500,\n",
    "        save_steps=500,\n",
    "        save_strategy=\"steps\",\n",
    "        learning_rate=3e-4,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": True},\n",
    "        load_best_model_at_end=True,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    print(\"🚀 STARTING SFT TRAINING\")\n",
    "    trainer.train()\n",
    "    model = trainer.model.merge_and_unload()\n",
    "    model.save_pretrained(f\"{MODEL_NAME}_lora_{DATASET_NAME}_model\")\n",
    "\n",
    "print(\"✅ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 1375107,
     "sourceId": 12500,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
